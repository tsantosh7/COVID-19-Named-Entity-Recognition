{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm \n",
    "from collections import defaultdict\n",
    "\n",
    "root_path ='/home/santosh/Work/Datasets/Datasets/Covid-19pubmed/'\n",
    "\n",
    "annotation_path = root_path+'annotations/'\n",
    "text_path = root_path+'raw/'\n",
    "\n",
    "# annotation_files = sorted(glob.glob(annotation_path + '*.a*'))\n",
    "text_files = sorted(glob.glob(text_path + '*.txt*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sci_spacy\n",
    "# https://allenai.github.io/scispacy/\n",
    "\n",
    "import scispacy\n",
    "import spacy\n",
    "# import en_core_web_sm\n",
    "import en_core_sci_sm\n",
    "nlp = en_core_sci_sm.load() # for sentencising. The best sentenciser for biomedical text\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17740/17740 [11:33<00:00, 25.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# get datasets for different entity type as well as datasets with all the entities in one.\n",
    "non_overlapping_dataset = defaultdict(list) # the entities are non overlapping here\n",
    "overlapping_dataset = defaultdict(list) \n",
    "Disorder_dataset= defaultdict(list) #(DISO)\n",
    "Species_dataset= defaultdict(list) #(SPEC)\n",
    "Chemical_Drug_dataset= defaultdict(list) #(CHED)\n",
    "Gene_Protein_dataset = defaultdict(list)#(PRGE)\n",
    "Enzyme_dataset = defaultdict(list)#(ENZY)\n",
    "Anatomy_dataset = defaultdict(list)#(ANAT)\n",
    "Biological_Process_dataset = defaultdict(list)#(PROC)\n",
    "Molecular_Function_dataset = defaultdict(list)#(FUNC)\n",
    "Cellular_Component_dataset = defaultdict(list)#(COMP)\n",
    "Pathway_dataset = defaultdict(list)#(PATH)\n",
    "microRNA_dataset = defaultdict(list) #(MRNA)\n",
    "\n",
    "# iter = 0\n",
    "for each_text_path in tqdm(text_files):\n",
    "\n",
    "#     iter = iter+1\n",
    "#     if iter==10:\n",
    "#         break\n",
    "    annotation_list =[] # list of entities from the annotations file\n",
    "    text_list = [] # sentences list from the text file, with sentence spans\n",
    "    non_overlapping_annotation_list = [] # list with non overlapping entities\n",
    "    \n",
    "    # read the text file \n",
    "    with open(each_text_path) as text_file:\n",
    "        text = text_file.read()\n",
    "        # sentencise the text and get the spans\n",
    "        doc = nlp(text)\n",
    "        for sent_ in doc.sents:\n",
    "            sent = str(sent_).replace('\\n',' ')\n",
    "            text_list.append([sent, sent_.start_char, sent_.end_char])\n",
    "    \n",
    "    # read the annotations for the respective raw text file\n",
    "    each_annotation_path = annotation_path+each_text_path.split('/')[-1].replace('txt','a1')\n",
    "    \n",
    "#     print(each_text_path)\n",
    "#     print(each_annotation_path)\n",
    "    \n",
    "    with open(each_annotation_path) as annotation_file:\n",
    "        annotation = annotation_file.readlines()\n",
    "    \n",
    "    # load the anotations into a list. There is a small problem here due to data formating consistencies.\n",
    "    for each_line in annotation:\n",
    "        temp_ = each_line.split()\n",
    "        if 'T' in temp_[0]:\n",
    "            if len(temp_)>5:\n",
    "                merged_anno = temp_[0:4]+ [' '.join(temp_[4:])] # because of the general split we will have to merge entities with spaces together\n",
    "                annotation_list.append(merged_anno)\n",
    "            \n",
    "            else:\n",
    "                annotation_list.append(temp_)\n",
    "\n",
    "                # Remove the overlaps\n",
    "    for inx in range(-1, len(annotation_list)):\n",
    "        if inx == len(annotation_list)-1:\n",
    "            break\n",
    "        if inx ==-1:\n",
    "            non_overlapping_annotation_list.append(annotation_list[inx+1])\n",
    "        else:\n",
    "            if annotation_list[inx+1][2]>annotation_list[inx][3]:\n",
    "                non_overlapping_annotation_list.append(annotation_list[inx+1])\n",
    "            \n",
    "\n",
    "    # we need to remove the off-set for the sentence span to reflect the span of the entities at the sentence level rather than at the document level\n",
    "    for each_annotation in annotation_list:    \n",
    "            \n",
    "        st_ann_sp = int(each_annotation[2]) #  start of annotation span\n",
    "        en_ann_sp = int(each_annotation[3]) #  end of annotation span\n",
    "        ann_type = each_annotation[1] #  annotation type\n",
    "        ann = each_annotation[4] # annotation\n",
    "            \n",
    "        for each_text in text_list:\n",
    "            \n",
    "            snt_text = str(each_text[0]) # sentence text\n",
    "            st_snt_sp = int(each_text[1]) # Start of sentence span\n",
    "            en_snt_sp = int(each_text[2]) # end of sentence span\n",
    "            \n",
    "                # check if the annotation span is with in the sentence span \n",
    "            if st_snt_sp <= st_ann_sp <= en_snt_sp and st_snt_sp <= en_ann_sp <= en_snt_sp:\n",
    "                if ann in snt_text: # process only if the annotation is in the sentence \n",
    "                    anno_list = [st_ann_sp-st_snt_sp, en_ann_sp-st_snt_sp,ann, ann_type] # get the annotation details\n",
    "                    if ann_type == 'SPEC':\n",
    "                        Species_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'DISO':\n",
    "                        Disorder_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'CHED':\n",
    "                        Chemical_Drug_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'PRGE':\n",
    "                        Gene_Protein_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'ENZY':\n",
    "                        Enzyme_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'ANAT':\n",
    "                        Anatomy_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'PROC':\n",
    "                        Biological_Process_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'FUNC':\n",
    "                        Molecular_Function_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'COMP':\n",
    "                        Cellular_Component_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'PATH':\n",
    "                        Pathway_dataset[snt_text].append(anno_list)\n",
    "                    elif ann_type == 'MRNA':\n",
    "                        microRNA_dataset[snt_text].append(anno_list)\n",
    "\n",
    "#                 overlapping_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]])                                                        \n",
    "                \n",
    "                # check if the annotations have overlap, the entities span in sentence should be greater than the previous one\n",
    "    \n",
    "    for each_annotation in non_overlapping_annotation_list:    \n",
    "\n",
    "        st_ann_sp = int(each_annotation[2]) #  start of annotation span\n",
    "        en_ann_sp = int(each_annotation[3]) #  end of annotation span\n",
    "        ann_type = each_annotation[1] #  annotation type\n",
    "        ann = each_annotation[4] # annotation\n",
    "\n",
    "        for each_text in text_list:\n",
    "\n",
    "            snt_text = str(each_text[0]) # sentence text\n",
    "            st_snt_sp = int(each_text[1]) # Start of sentence span\n",
    "            en_snt_sp = int(each_text[2]) # end of sentence span\n",
    "            \n",
    "            if st_snt_sp <= st_ann_sp <= en_snt_sp and st_snt_sp <= en_ann_sp <= en_snt_sp:\n",
    "                if ann in snt_text: # process only if the annotation is in the sentence \n",
    "                    anno_list = [st_ann_sp-st_snt_sp, en_ann_sp-st_snt_sp,ann, ann_type] # get the annotation details\n",
    "                    non_overlapping_dataset[snt_text].append(anno_list)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical and physiological data, serum concentrations of tumour necrosis factor (TNF)-alpha, interleukin (IL)-1beta and IL-6, and quantitative cultures of lower respiratory tract specimens were obtained from 46 patients with ARDS and 20 with severe pneumonia within 24 hours of the onset of the disease and from 10 control subjects with no inflammatory lung disease. [[93, 104, 'interleukin', 'PRGE'], [120, 124, 'IL-6', 'PRGE']]\n"
     ]
    }
   ],
   "source": [
    "for k,v in Gene_Protein_dataset.items():\n",
    "    print(k,v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train, test and dev pmc ids\n",
    "import math\n",
    "import random\n",
    "import jsonlines\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def get_train_dev_test_indxs(total_num_annotations):\n",
    "\n",
    "    percentage=0.70\n",
    "    iter = 0\n",
    "\n",
    "    trainids = []\n",
    "    devids = []\n",
    "    testids =[]\n",
    "\n",
    "    nLines = total_num_annotations\n",
    "    nTrain = int(nLines*percentage) \n",
    "    nValid = math.floor((nLines - nTrain)/2)\n",
    "    nTest = nLines - (nTrain+nValid)\n",
    "\n",
    "    deck = list(range(0, nLines))\n",
    "    random.seed(45) # This will be fixed for reproducibility\n",
    "    random.shuffle(deck)\n",
    "\n",
    "    train_ids = deck[0:nTrain]\n",
    "    devel_ids = deck[nTrain:nTrain+nValid]\n",
    "    test_ids = deck[nTrain+nValid:nTrain+nValid+nTest]\n",
    "\n",
    "    return train_ids, devel_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_jsonl_format(dictionary_dataset, train_ids, devel_ids, test_ids,path, folder_name): \n",
    "    \n",
    "    result_path = cwd_+folder_name\n",
    "    pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_jsonl_data = []\n",
    "    devel_jsonl_data=[]\n",
    "    test_jsonl_data = []\n",
    "    \n",
    "    iter = 1\n",
    "    \n",
    "    for key, values in tqdm(dictionary_dataset.items(), total=len(dictionary_dataset)):\n",
    "        \n",
    "        text = str(key)\n",
    "        entities =[]\n",
    "\n",
    "        for each_ner in values:\n",
    "            point_start = int(each_ner[0])\n",
    "            point_end = int(each_ner[1])\n",
    "            label = each_ner[3]\n",
    "            entities.append((point_start, point_end,label))\n",
    "        \n",
    "        if iter in train_ids:\n",
    "            train_jsonl_data.append((text, {\"entities\" : entities}))\n",
    "        elif iter in devel_ids:\n",
    "            devel_jsonl_data.append((text, {\"entities\" : entities}))\n",
    "        elif iter in test_ids:\n",
    "            test_jsonl_data.append((text, {\"entities\" : entities}))            \n",
    "    \n",
    "        iter = iter+1\n",
    "        \n",
    "    with jsonlines.open(result_path+'train.json', mode='w') as writer:\n",
    "        for each_line in test_jsonl_data:\n",
    "            writer.write(each_line)\n",
    "\n",
    "    with jsonlines.open(result_path+'devel.json', mode='w') as writer:\n",
    "        for each_line in devel_jsonl_data:\n",
    "            writer.write(each_line)\n",
    "            \n",
    "    with jsonlines.open(result_path+'test.json', mode='w') as writer:\n",
    "        for each_line in test_jsonl_data:\n",
    "            writer.write(each_line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # if a sub span is present in full span return it\n",
    "    if sub_span_range[0] in range(full_spans_range[0],full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "\n",
    "    \n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "#     print(text_data, ner_tags)\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    spans = []\n",
    "\n",
    "    split_text = tokenizer.tokenize(text_data)\n",
    "    span_text = list(tokenizer.span_tokenize(text_data))\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O'] * len(split_text)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        span_list = (int(each_tag[0]), int(each_tag[1]))\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if (len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "        for i in range(0, len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if (i == 0):\n",
    "                ner_list[i] = 'B-' + ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-' + ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        spans.append(span_list)\n",
    "\n",
    "    split_token_span_list = list(zip(split_text, span_text))\n",
    "    span_ner_list = list(zip(spans, ners))\n",
    "\n",
    "    \n",
    "    sub_spans =[] # get sub spans from the full spans of the ner\n",
    "\n",
    "    for each_span_ner_list in span_ner_list:\n",
    "        # in full range ner e.g., [144, 150, 'COVID-19', 'DISO']\n",
    "        count = 0\n",
    "        # count is to keep track of the B, I, sub tags in the ner list\n",
    "        for each_token in split_token_span_list:\n",
    "            sub_spans_ = find_sub_span(each_token[1], each_span_ner_list[0])\n",
    "            if sub_spans_:\n",
    "                sub_spans.append([sub_spans_,each_span_ner_list[1][count]])\n",
    "                count = count+1\n",
    "            \n",
    "            \n",
    "    \n",
    "    for i, each_span_token in enumerate(split_token_span_list):\n",
    "        for each_ner_span in sub_spans:\n",
    "            if each_span_token[1] == each_ner_span[0]:\n",
    "                arr[i] = ''.join(each_ner_span[1])\n",
    "\n",
    "    return zip(split_text, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_IOB_format(dictionary_dataset, train_ids, devel_ids, test_ids, path, folder_name):\n",
    "    \n",
    "    result_path = cwd_+folder_name\n",
    "    pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(result_path + 'train.csv', 'w', newline='\\n') as f1, open(result_path + 'devel.csv', 'w',\n",
    "                        newline='\\n') as f2, open(result_path + 'test.csv', 'w', newline='\\n') as f3:\n",
    "\n",
    "        train_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "        dev_writer = csv.writer(f2, delimiter='\\t', lineterminator='\\n')\n",
    "        test_writer = csv.writer(f3, delimiter='\\t', lineterminator='\\n')\n",
    "        \n",
    "        iter = 0\n",
    "\n",
    "        for key, values in tqdm(dictionary_dataset.items(), total=len(dictionary_dataset)):\n",
    "            \n",
    "            text = str(key)\n",
    "#             print(text)\n",
    "            tagged_tokens = convert2IOB(text, values)\n",
    "\n",
    "\n",
    "            if iter in train_ids:\n",
    "                for each_token in tagged_tokens:\n",
    "                    train_writer.writerow(list(each_token))\n",
    "                train_writer.writerow('')\n",
    "\n",
    "            elif iter in devel_ids:\n",
    "                for each_token in tagged_tokens:\n",
    "                    dev_writer.writerow(list(each_token))\n",
    "                dev_writer.writerow('')\n",
    "\n",
    "            elif iter in test_ids:\n",
    "                for each_token in tagged_tokens:\n",
    "                    test_writer.writerow(list(each_token))\n",
    "                test_writer.writerow('')\n",
    "            \n",
    "            iter = iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/santosh/Work/GitHub/COVID-19-Named-Entity-Recognition-/Datasets/\n"
     ]
    }
   ],
   "source": [
    "cwd_ = os.getcwd()+'/Datasets/'\n",
    "print(cwd_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_overlapping_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 79192/156907 [02:49<03:59, 323.86it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a2fcb6843c11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'non_overlapping_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_dev_test_indxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_overlapping_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconvert_to_IOB_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_overlapping_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BIO/non_overlapping_dataset/'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconvert_to_jsonl_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_overlapping_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SpaCy/non_overlapping_dataset/'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3ba7f09ce8f7>\u001b[0m in \u001b[0;36mconvert_to_IOB_format\u001b[0;34m(dictionary_dataset, train_ids, devel_ids, test_ids, path, folder_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#             print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert2IOB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7324b89cc928>\u001b[0m in \u001b[0;36mconvert2IOB\u001b[0;34m(text_data, ner_tags)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0msub_spans_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_sub_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_span_ner_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msub_spans_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0msub_spans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub_spans_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meach_span_ner_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 79192/156907 [03:00<03:59, 323.86it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "non_overlapping_dataset \n",
    "print('non_overlapping_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(non_overlapping_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/non_overlapping_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/non_overlapping_dataset/' )\n",
    "\n",
    "\n",
    "# overlapping_dataset\n",
    "# trainidx, develidx, testidx = get_train_dev_test_indxs(len(overlapping_dataset))\n",
    "# convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/overlapping_dataset/' )\n",
    "# convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/overlapping_dataset/' )\n",
    "\n",
    "# Disorder_dataset\n",
    "print('Disorder_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Disorder_dataset))\n",
    "convert_to_IOB_format(Disorder_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Disorder_dataset/' )\n",
    "convert_to_jsonl_format(Disorder_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Disorder_dataset/' )\n",
    "\n",
    "# Species_dataset\n",
    "print('Species_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Species_dataset))\n",
    "convert_to_IOB_format(Species_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Species_dataset/' )\n",
    "convert_to_jsonl_format(Species_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Species_dataset/' )\n",
    "\n",
    "# Chemical_Drug_dataset\n",
    "print('Chemical_Drug_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Chemical_Drug_dataset))\n",
    "convert_to_IOB_format(Chemical_Drug_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Chemical_Drug_dataset/' )\n",
    "convert_to_jsonl_format(Chemical_Drug_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Chemical_Drug_dataset/' )\n",
    "\n",
    "# Gene_Protein_dataset\n",
    "print('Gene_Protein_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Gene_Protein_dataset))\n",
    "convert_to_IOB_format(Gene_Protein_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Gene_Protein_dataset/' )\n",
    "convert_to_jsonl_format(Gene_Protein_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Gene_Protein_dataset/' )\n",
    "\n",
    "# Enzyme_dataset\n",
    "print('Enzyme_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Enzyme_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Enzyme_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Enzyme_dataset/' )\n",
    "\n",
    "# Anatomy_dataset\n",
    "print('Anatomy_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Anatomy_dataset))\n",
    "convert_to_IOB_format(Enzyme_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Anatomy_dataset/' )\n",
    "convert_to_jsonl_format(Enzyme_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Anatomy_dataset/' )\n",
    "\n",
    "# Biological_Process_dataset\n",
    "print('Biological_Process_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Biological_Process_dataset))\n",
    "convert_to_IOB_format(Biological_Process_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Biological_Process_dataset/' )\n",
    "convert_to_jsonl_format(Biological_Process_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Biological_Process_dataset/' )\n",
    "\n",
    "# Molecular_Function_dataset\n",
    "print('Molecular_Function_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Molecular_Function_dataset))\n",
    "convert_to_IOB_format(Molecular_Function_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Molecular_Function_dataset/' )\n",
    "convert_to_jsonl_format(Molecular_Function_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Molecular_Function_dataset/' )\n",
    "\n",
    "# Cellular_Component_dataset\n",
    "print('Cellular_Component_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Cellular_Component_dataset))\n",
    "convert_to_IOB_format(Cellular_Component_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Cellular_Component_dataset/' )\n",
    "convert_to_jsonl_format(Cellular_Component_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Cellular_Component_dataset/' )\n",
    "\n",
    "# Pathway_dataset\n",
    "print('Pathway_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Pathway_dataset))\n",
    "convert_to_IOB_format(Pathway_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Pathway_dataset/' )\n",
    "convert_to_jsonl_format(Pathway_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Pathway_dataset/' )\n",
    "\n",
    "# microRNA_dataset\n",
    "print('microRNA_dataset')\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(microRNA_dataset))\n",
    "convert_to_IOB_format(microRNA_dataset,trainidx, develidx, testidx, cwd_, 'BIO/microRNA_dataset/' )\n",
    "convert_to_jsonl_format(microRNA_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/microRNA_dataset/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30324 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 355/30324 [00:00<00:08, 3542.15it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene_Protein_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|▏         | 702/30324 [00:00<00:08, 3518.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 999/30324 [00:00<00:08, 3332.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 1336/30324 [00:00<00:08, 3342.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 1665/30324 [00:00<00:08, 3325.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 1995/30324 [00:00<00:08, 3318.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 2286/30324 [00:00<00:08, 3164.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 2603/30324 [00:00<00:08, 3164.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 2906/30324 [00:00<00:08, 3122.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 3206/30324 [00:01<00:09, 2939.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3493/30324 [00:01<00:10, 2621.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 3758/30324 [00:01<00:10, 2629.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 4059/30324 [00:01<00:09, 2730.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 4362/30324 [00:01<00:09, 2813.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 4646/30324 [00:01<00:09, 2806.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 4928/30324 [00:01<00:09, 2805.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 5210/30324 [00:01<00:09, 2645.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 5478/30324 [00:01<00:09, 2627.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 5758/30324 [00:01<00:09, 2674.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 6035/30324 [00:02<00:08, 2700.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 6307/30324 [00:02<00:09, 2632.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 6572/30324 [00:02<00:09, 2595.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 6838/30324 [00:02<00:08, 2613.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 7126/30324 [00:02<00:08, 2688.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 7396/30324 [00:02<00:08, 2663.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 7664/30324 [00:02<00:08, 2614.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 7927/30324 [00:02<00:09, 2382.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 8170/30324 [00:02<00:09, 2334.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 8428/30324 [00:03<00:09, 2402.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 8684/30324 [00:03<00:08, 2445.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 8931/30324 [00:03<00:08, 2425.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 9176/30324 [00:03<00:09, 2262.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 9406/30324 [00:03<00:09, 2223.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 9642/30324 [00:03<00:09, 2260.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 9881/30324 [00:03<00:08, 2293.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 10112/30324 [00:03<00:09, 2207.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 10335/30324 [00:03<00:09, 2210.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 10596/30324 [00:04<00:08, 2315.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 10834/30324 [00:04<00:08, 2331.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11069/30324 [00:04<00:08, 2303.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 11301/30324 [00:04<00:08, 2301.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 11532/30324 [00:04<00:08, 2292.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 11771/30324 [00:04<00:07, 2320.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 12024/30324 [00:04<00:07, 2377.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 12263/30324 [00:04<00:07, 2350.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 12499/30324 [00:04<00:07, 2233.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 12724/30324 [00:04<00:08, 2195.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 12945/30324 [00:05<00:08, 2131.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 13172/30324 [00:05<00:07, 2168.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 13390/30324 [00:05<00:07, 2118.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 13603/30324 [00:05<00:08, 2059.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 13839/30324 [00:05<00:07, 2140.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▋     | 14082/30324 [00:05<00:07, 2219.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 14327/30324 [00:05<00:07, 2282.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 14568/30324 [00:05<00:06, 2317.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 14802/30324 [00:05<00:06, 2299.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 15039/30324 [00:05<00:06, 2317.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 15278/30324 [00:06<00:06, 2337.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 15513/30324 [00:06<00:06, 2329.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 15747/30324 [00:06<00:06, 2277.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 15976/30324 [00:06<00:06, 2251.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 16210/30324 [00:06<00:06, 2276.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 16445/30324 [00:06<00:06, 2293.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 16678/30324 [00:06<00:05, 2300.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 16922/30324 [00:06<00:05, 2339.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17157/30324 [00:06<00:05, 2283.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 17401/30324 [00:07<00:05, 2326.44it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3dad23ed04b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#             print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert2IOB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7324b89cc928>\u001b[0m in \u001b[0;36mconvert2IOB\u001b[0;34m(text_data, ner_tags)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0msub_spans_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_sub_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_span_ner_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msub_spans_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0msub_spans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub_spans_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meach_span_ner_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Gene_Protein_dataset')\n",
    "train_ids, devel_ids, test_ids = get_train_dev_test_indxs(len(Gene_Protein_dataset))\n",
    "\n",
    "result_path = cwd_+'BIO/Gene_Protein_dataset/'\n",
    "\n",
    "with open(result_path + 'train.csv', 'w', newline='\\n') as f1, open(result_path + 'devel.csv', 'w',\n",
    "                    newline='\\n') as f2, open(result_path + 'test.csv', 'w', newline='\\n') as f3:\n",
    "\n",
    "    train_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "    dev_writer = csv.writer(f2, delimiter='\\t', lineterminator='\\n')\n",
    "    test_writer = csv.writer(f3, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "    iter = 0\n",
    "\n",
    "    for key, values in tqdm(Gene_Protein_dataset.items(), total=len(Gene_Protein_dataset)):\n",
    "\n",
    "        text = str(key)\n",
    "#             print(text)\n",
    "        tagged_tokens = convert2IOB(text, values)\n",
    "\n",
    "\n",
    "        if iter in train_ids:\n",
    "            for each_token in tagged_tokens:\n",
    "                train_writer.writerow(list(each_token))\n",
    "            train_writer.writerow('')\n",
    "\n",
    "        elif iter in devel_ids:\n",
    "            for each_token in tagged_tokens:\n",
    "                dev_writer.writerow(list(each_token))\n",
    "            dev_writer.writerow('')\n",
    "\n",
    "        elif iter in test_ids:\n",
    "            for each_token in tagged_tokens:\n",
    "                test_writer.writerow(list(each_token))\n",
    "            test_writer.writerow('')\n",
    "\n",
    "        iter = iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_text_path = '/home/santosh/Work/Datasets/Datasets/Covid-19pubmed/raw/23202509.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file \n",
    "with open(each_text_path) as text_file:\n",
    "    text = text_file.read()\n",
    "    # sentencise the text and get the spans\n",
    "    doc = nlp(text)\n",
    "    for sent_ in doc.sents:\n",
    "        sent = str(sent_).replace('\\n',' ')\n",
    "        text_list.append([sent, sent_.start_char, sent_.end_char])\n",
    "\n",
    "# read the annotations for the respective raw text file\n",
    "each_annotation_path = annotation_path+each_text_path.split('/')[-1].replace('txt','a1')\n",
    "\n",
    "#     print(each_text_path)\n",
    "#     print(each_annotation_path)\n",
    "\n",
    "with open(each_annotation_path) as annotation_file:\n",
    "    annotation = annotation_file.readlines()\n",
    "\n",
    "# load the anotations into a list. There is a small problem here due to data formating consistencies.\n",
    "for each_line in annotation:\n",
    "    temp_ = each_line.split()\n",
    "    if 'T' in temp_[0]:\n",
    "        if len(temp_)>5:\n",
    "            merged_anno = temp_[0:4]+ [' '.join(temp_[4:])] # because of the general split we will have to merge entities with spaces together\n",
    "            annotation_list.append(merged_anno)\n",
    "\n",
    "        else:\n",
    "            annotation_list.append(temp_)\n",
    "\n",
    "            # Remove the overlaps\n",
    "for inx in range(-1, len(annotation_list)):\n",
    "    if inx == len(annotation_list)-1:\n",
    "        break\n",
    "    if inx ==-1:\n",
    "        non_overlapping_annotation_list.append(annotation_list[inx+1])\n",
    "    else:\n",
    "        if annotation_list[inx+1][2]>annotation_list[inx][3]:\n",
    "            non_overlapping_annotation_list.append(annotation_list[inx+1])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_sci_sm\n",
    "nlp = en_core_sci_sm.load() # for sentencising. The best sentenciser for biomedical text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "doc = nlp(text)\n",
    "for sent_ in doc.sents:\n",
    "    sent = str(sent_).replace('\\n',' ')\n",
    "    text_list.append([sent, sent_.start_char, sent_.end_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['TITLE: The role of severe acute respiratory syndrome (SARS)-coronavirus accessory proteins in virus pathogenesis.  ',\n",
       "  0,\n",
       "  115],\n",
       " ['ABSTRACT: A respiratory disease caused by a novel coronavirus, termed the severe acute respiratory syndrome coronavirus (SARS-CoV), was first reported in China in late 2002.',\n",
       "  115,\n",
       "  288],\n",
       " ['The subsequent efficient human-to-human transmission of this virus eventually affected more than 30 countries worldwide, resulting in a mortality rate of &#126;10% of infected individuals.',\n",
       "  289,\n",
       "  477],\n",
       " ['The spread of the virus was ultimately controlled by isolation of infected individuals and there has been no infections reported since April 2004.',\n",
       "  478,\n",
       "  624],\n",
       " ['However, the natural reservoir of the virus was never identified and it is not known if this virus will  re-emerge and, therefore, research on this virus continues.',\n",
       "  625,\n",
       "  789],\n",
       " ['The SARS-CoV genome is about 30 kb in length and is predicted to contain 14 functional open reading  frames (ORFs).',\n",
       "  790,\n",
       "  905],\n",
       " ['The genome encodes for proteins that are homologous to known coronavirus proteins, such as the replicase proteins (ORFs 1a and 1b) and the four major structural proteins: nucleocapsid (N), spike (S), membrane (M) and envelope (E).  ',\n",
       "  906,\n",
       "  1138],\n",
       " ['SARS-CoV also encodes for eight unique proteins, called accessory proteins, with no known homologues.',\n",
       "  1138,\n",
       "  1239],\n",
       " ['This review will summarize the current knowledge on SARS-CoV accessory proteins and will include: (i) expression and processing; (ii) the effects on cellular processes; and (iii) functional studies.   ',\n",
       "  1240,\n",
       "  1441]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['T0', 'CHED', '20', '31', 'Chloroquine'],\n",
       " ['T1', 'DISO', '52', '77', 'Corona Virus Disease 2019'],\n",
       " ['T2', 'CHED', '52', '58', 'Corona'],\n",
       " ['T3', 'SPEC', '52', '58', 'Corona'],\n",
       " ['T4', 'PROC', '59', '72', 'Virus Disease'],\n",
       " ['T5', 'SPEC', '59', '64', 'Virus'],\n",
       " ['T6', 'DISO', '78', '85', '（COVID-'],\n",
       " ['T7', 'CHED', '150', '161', 'Chloroquine'],\n",
       " ['T8', 'PATH', '241', '248', 'malaria'],\n",
       " ['T9', 'DISO', '241', '248', 'malaria'],\n",
       " ['T10', 'DISO', '253', '279', 'connective tissue diseases'],\n",
       " ['T11', 'ANAT', '253', '270', 'connective tissue'],\n",
       " ['T12', 'DISO', '302', '310', 'COVID-19'],\n",
       " ['T13', 'DISO', '311', '332', '（corona virus disease'],\n",
       " ['T14', 'CHED', '311', '318', '（corona'],\n",
       " ['T15', 'SPEC', '311', '318', '（corona'],\n",
       " ['T16', 'PROC', '319', '332', 'virus disease'],\n",
       " ['T17', 'SPEC', '319', '324', 'virus'],\n",
       " ['T18', 'SPEC', '359', '369', 'SARS-CoV-2'],\n",
       " ['T19', 'DISO', '359', '363', 'SARS'],\n",
       " ['T20',\n",
       "  'SPEC',\n",
       "  '370',\n",
       "  '416',\n",
       "  '（severe acute respiratory syndrome coronavirus'],\n",
       " ['T21', 'DISO', '370', '404', '（severe acute respiratory syndrome'],\n",
       " ['T22', 'DISO', '512', '520', 'COVID-19'],\n",
       " ['T23', 'SPEC', '627', '637', 'SARS-CoV-2'],\n",
       " ['T24', 'DISO', '627', '631', 'SARS'],\n",
       " ['T25', 'CHED', '671', '683', 'antimalarial'],\n",
       " ['T26', 'CHED', '689', '700', 'Chloroquine'],\n",
       " ['T27', 'CHED', '701', '710', 'phosphate'],\n",
       " ['T28', 'PRGE', '767', '782', 'anti-SARS-CoV-2'],\n",
       " ['T29', 'SPEC', '772', '782', 'SARS-CoV-2'],\n",
       " ['T30', 'DISO', '772', '776', 'SARS'],\n",
       " ['T31', 'CHED', '888', '899', 'chloroquine'],\n",
       " ['T32', 'CHED', '900', '909', 'phosphate'],\n",
       " ['T33', 'PROC', '942', '947', 'death'],\n",
       " ['T34', 'PATH', '1107', '1114', 'malaria'],\n",
       " ['T35', 'DISO', '1107', '1114', 'malaria'],\n",
       " ['T36', 'PROC', '1289', '1299', 'metabolism'],\n",
       " ['T37', 'CHED', '1341', '1352', 'chloroquine'],\n",
       " ['T38', 'SPEC', '1363', '1368', 'order'],\n",
       " ['T0', 'DISO', '19', '52', 'severe acute respiratory syndrome'],\n",
       " ['T1', 'DISO', '54', '58', 'SARS'],\n",
       " ['T2', 'SPEC', '60', '71', 'coronavirus'],\n",
       " ['T3', 'CHED', '82', '90', 'proteins'],\n",
       " ['T4', 'DISO', '94', '112', 'virus pathogenesis'],\n",
       " ['T5', 'SPEC', '94', '99', 'virus'],\n",
       " ['T6', 'PROC', '100', '112', 'pathogenesis'],\n",
       " ['T7', 'DISO', '127', '146', 'respiratory disease'],\n",
       " ['T8', 'SPEC', '159', '176', 'novel coronavirus'],\n",
       " ['T9', 'DISO', '189', '222', 'severe acute respiratory syndrome'],\n",
       " ['T10', 'SPEC', '223', '234', 'coronavirus'],\n",
       " ['T11', 'DISO', '236', '240', 'SARS'],\n",
       " ['T12', 'SPEC', '314', '319', 'human'],\n",
       " ['T13', 'SPEC', '323', '328', 'human'],\n",
       " ['T14', 'SPEC', '350', '355', 'virus'],\n",
       " ['T15', 'SPEC', '491', '496', 'virus'],\n",
       " ['T16', 'DISO', '582', '592', 'infections'],\n",
       " ['T17', 'SPEC', '658', '663', 'virus'],\n",
       " ['T18', 'SPEC', '713', '718', 'virus'],\n",
       " ['T19', 'SPEC', '768', '773', 'virus'],\n",
       " ['T20', 'DISO', '789', '793', 'SARS'],\n",
       " ['T21', 'CHED', '924', '932', 'proteins'],\n",
       " ['T22', 'SPEC', '962', '973', 'coronavirus'],\n",
       " ['T23', 'CHED', '974', '982', 'proteins'],\n",
       " ['T24', 'PRGE', '996', '1014', 'replicase proteins'],\n",
       " ['T25', 'CHED', '1006', '1014', 'proteins'],\n",
       " ['T26', 'PRGE', '1016', '1030', 'ORFs 1a and 1b'],\n",
       " ['T27', 'CHED', '1062', '1070', 'proteins'],\n",
       " ['T28', 'COMP', '1072', '1084', 'nucleocapsid'],\n",
       " ['T29', 'ANAT', '1072', '1084', 'nucleocapsid'],\n",
       " ['T30', 'COMP', '1101', '1109', 'membrane'],\n",
       " ['T31', 'ANAT', '1101', '1109', 'membrane'],\n",
       " ['T32', 'COMP', '1118', '1126', 'envelope'],\n",
       " ['T33', 'PRGE', '1118', '1126', 'envelope'],\n",
       " ['T34', 'ANAT', '1118', '1126', 'envelope'],\n",
       " ['T35', 'DISO', '1133', '1137', 'SARS'],\n",
       " ['T36', 'CHED', '1172', '1180', 'proteins'],\n",
       " ['T37', 'CHED', '1199', '1207', 'proteins'],\n",
       " ['T38', 'PRGE', '1287', '1314', 'SARS-CoV accessory proteins'],\n",
       " ['T39', 'DISO', '1287', '1291', 'SARS'],\n",
       " ['T40', 'CHED', '1306', '1314', 'proteins'],\n",
       " ['T41', 'PROC', '1337', '1347', 'expression'],\n",
       " ['T42', 'COMP', '1384', '1392', 'cellular'],\n",
       " ['T43', 'ANAT', '1384', '1392', 'cellular']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TITLE:\\nThe role of severe acute respiratory syndrome (SARS)-coronavirus accessory proteins in virus pathogenesis.\\n\\nABSTRACT:\\nA respiratory disease caused by a novel coronavirus, termed the severe acute respiratory syndrome coronavirus (SARS-CoV), was first reported in China in late 2002. The subsequent efficient human-to-human transmission of this virus eventually affected more than 30 countries worldwide, resulting in a mortality rate of &#126;10% of infected individuals. The spread of the virus was ultimately controlled by isolation of infected individuals and there has been no infections reported since April 2004. However, the natural reservoir of the virus was never identified and it is not known if this virus will  re-emerge and, therefore, research on this virus continues. The SARS-CoV genome is about 30 kb in length and is predicted to contain 14 functional open reading  frames (ORFs). The genome encodes for proteins that are homologous to known coronavirus proteins, such as the replicase proteins (ORFs 1a and 1b) and the four major structural proteins: nucleocapsid (N), spike (S), membrane (M) and envelope (E).  SARS-CoV also encodes for eight unique proteins, called accessory proteins, with no known homologues. This review will summarize the current knowledge on SARS-CoV accessory proteins and will include: (i) expression and processing; (ii) the effects on cellular processes; and (iii) functional studies.\\n\\n\\n',\n",
       " [[90, 108, 'replicase proteins', 'PRGE'],\n",
       "  [110, 124, 'ORFs 1a and 1b', 'PRGE'],\n",
       "  [212, 220, 'envelope', 'PRGE']])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scispacy]",
   "language": "python",
   "name": "conda-env-scispacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
