{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm import tqdm \n",
    "from collections import defaultdict\n",
    "\n",
    "root_path ='/home/santosh/Work/Datasets/Datasets/Covid-19pubmed/'\n",
    "\n",
    "annotation_path = root_path+'annotations/'\n",
    "text_path = root_path+'raw/'\n",
    "\n",
    "# annotation_files = sorted(glob.glob(annotation_path + '*.a*'))\n",
    "text_files = sorted(glob.glob(text_path + '*.txt*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sci_spacy\n",
    "# https://allenai.github.io/scispacy/\n",
    "\n",
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_sm\n",
    "nlp = en_core_sci_sm.load() # for sentencising. The best sentenciser for biomedical text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17740 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# get datasets for different entity type as well as datasets with all the entities in one.\n",
    "non_overlapping_dataset = defaultdict(list) # the entities are non overlapping here\n",
    "overlapping_dataset = defaultdict(list) \n",
    "Disorder_dataset= defaultdict(list) #(DISO)\n",
    "Species_dataset= defaultdict(list) #(SPEC)\n",
    "Chemical_Drug_dataset= defaultdict(list) #(CHED)\n",
    "Gene_Protein_dataset = defaultdict(list)#(PRGE)\n",
    "Enzyme_dataset = defaultdict(list)#(ENZY)\n",
    "Anatomy_dataset = defaultdict(list)#(ANAT)\n",
    "Biological_Process_dataset = defaultdict(list)#(PROC)\n",
    "Molecular_Function_dataset = defaultdict(list)#(FUNC)\n",
    "Cellular_Component_dataset = defaultdict(list)#(COMP)\n",
    "Pathway_dataset = defaultdict(list)#(PATH)\n",
    "microRNA_dataset = defaultdict(list) #(MRNA)\n",
    "\n",
    "iter = 0\n",
    "for each_text_path in tqdm(text_files):\n",
    "\n",
    "    iter = iter+1\n",
    "    if iter==3:\n",
    "        break\n",
    "    annotation_list =[] # list of entities from the annotations file\n",
    "    text_list = [] # sentences list from the text file, with sentence spans\n",
    "    non_overlapping_annotation_list = [] # list with non overlapping entities\n",
    "    \n",
    "    # read the text file \n",
    "    with open(each_text_path) as text_file:\n",
    "        text = text_file.read()\n",
    "        # sentencise the text and get the spans\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            text_list.append([sent, sent.start_char, sent.end_char])\n",
    "    \n",
    "    # read the annotations for the respective raw text file\n",
    "    each_annotation_path = annotation_path+text_files[0].split('/')[-1].replace('txt','a1')\n",
    "    \n",
    "    with open(each_annotation_path) as annotation_file:\n",
    "        annotation = annotation_file.readlines()\n",
    "    \n",
    "    # load the anotations into a list. There is a small problem here due to data formating consistencies.\n",
    "    for each_line in annotation:\n",
    "        temp_ = each_line.split()\n",
    "        if 'T' in temp_[0]:\n",
    "            if len(temp_)>5:\n",
    "                merged_anno = temp_[0:4]+ [' '.join(temp_[4:])] # because of the general split we will have to merge entities with spaces together\n",
    "                annotation_list.append(merged_anno)\n",
    "            \n",
    "            else:\n",
    "                annotation_list.append(temp_)\n",
    "\n",
    "                # Remove the overlaps\n",
    "    for inx in range(-1, len(annotation_list)):\n",
    "        if inx == len(annotation_list)-1:\n",
    "            break\n",
    "        if inx ==-1:\n",
    "            non_overlapping_annotation_list.append(annotation_list[inx+1])\n",
    "        else:\n",
    "            if annotation_list[inx+1][2]>annotation_list[inx][3]:\n",
    "                non_overlapping_annotation_list.append(annotation_list[inx+1])\n",
    "            \n",
    "\n",
    "    # we need to remove the off-set for the sentence span to reflect the span of the entities at the sentence level rather than at the document level\n",
    "    for each_annotation in annotation_list:    \n",
    "        for each_text in text_list:\n",
    "            if each_annotation[1] == 'SPEC':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Species_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'DISO':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Disorder_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'CHED':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Chemical_Drug_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'PRGE':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Gene_Protein_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'ENZY':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Enzyme_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'ANAT':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Anatomy_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'PROC':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Biological_Process_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'FUNC':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Molecular_Function_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'COMP':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Cellular_Component_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'PATH':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    Pathway_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            elif each_annotation[1] == 'MRNA':\n",
    "                if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                    microRNA_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]]) \n",
    "            \n",
    "            if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                overlapping_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]])                                                        \n",
    "                \n",
    "                # check if the annotations have overlap, the entities span in sentence should be greater than the previous one\n",
    "    for each_text in text_list:\n",
    "        for each_annotation in non_overlapping_annotation_list:\n",
    "            if int(each_text[1]) <= int(each_annotation[2]) <= int(each_text[2]) and int(each_text[1]) <= int(each_annotation[3]) <= int(each_text[2]):\n",
    "                non_overlapping_dataset[each_text[0]].append([int(each_annotation[2])-int(each_text[1]), int(each_annotation[3])-int(each_text[1]),each_annotation[4], each_annotation[1]])                                                        \n",
    "                    \n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {TITLE:\n",
       "             Identification of a bovine coronavirus packaging signal.\n",
       "             : [[27, 33, 'bovine', 'SPEC'], [34, 45, 'coronavirus', 'SPEC']],\n",
       "             ABSTRACT:\n",
       "             A region of the bovine coronavirus (BCV) genome that functions as a packaging signal has been cloned.: [[26,\n",
       "               32,\n",
       "               'bovine',\n",
       "               'SPEC'],\n",
       "              [33, 44, 'coronavirus', 'SPEC'],\n",
       "              [46, 49, 'BCV', 'SPEC']],\n",
       "             The 291-nucleotide clone shares 72% homology with the region of mouse hepatitis coronavirus (MHV) gene 1b that contains the packaging signal.: [[64,\n",
       "               69,\n",
       "               'mouse',\n",
       "               'SPEC'],\n",
       "              [80, 91, 'coronavirus', 'SPEC'],\n",
       "              [93, 96, 'MHV', 'SPEC']],\n",
       "             RNA transcripts were packaged into both BCV and MHV virions when the cloned region was appended to a noncoronavirus RNA.: [[40,\n",
       "               43,\n",
       "               'BCV',\n",
       "               'SPEC'],\n",
       "              [48, 51, 'MHV', 'SPEC']],\n",
       "             This is the first identification of a BCV packaging signal.: [[38,\n",
       "               41,\n",
       "               'BCV',\n",
       "               'SPEC']],\n",
       "             The data demonstrate that the BCV genome contains a sequence that is conserved at both the sequence and functional levels, thus broadening our insight into coronavirus packaging.\n",
       "             \n",
       "             : [[30, 33, 'BCV', 'SPEC'], [156, 167, 'coronavirus', 'SPEC']],\n",
       "             TITLE:\n",
       "             Comparison of systemic cytokine levels in patients with acute respiratory distress syndrome, severe pneumonia, and controls.\n",
       "             : [[27, 33, 'bovine', 'SPEC'],\n",
       "              [34, 45, 'coronavirus', 'SPEC'],\n",
       "              [91, 97, 'bovine', 'SPEC'],\n",
       "              [98, 109, 'coronavirus', 'SPEC'],\n",
       "              [111, 114, 'BCV', 'SPEC']],\n",
       "             ABSTRACT:\n",
       "             The inflammatory response has been widely investigated in patients with acute respiratory distress syndrome (ARDS) and pneumonia.: [[108,\n",
       "               113,\n",
       "               'mouse',\n",
       "               'SPEC'],\n",
       "              [124, 135, 'coronavirus', 'SPEC']],\n",
       "             Studies investigating the diagnostic values of serum cytokine levels have yielded conflicting results and only little information is available for the differential diagnosis between ARDS and pneumonia.: [[86,\n",
       "               89,\n",
       "               'BCV',\n",
       "               'SPEC'],\n",
       "              [94, 97, 'MHV', 'SPEC']],\n",
       "             Clinical and physiological data, serum concentrations of tumour necrosis factor (TNF)-alpha, interleukin (IL)-1beta and IL-6, and quantitative cultures of lower respiratory tract specimens were obtained from 46 patients with ARDS and 20 with severe pneumonia within 24 hours of the onset of the disease and from 10 control subjects with no inflammatory lung disease.: [[3,\n",
       "               6,\n",
       "               'BCV',\n",
       "               'SPEC'],\n",
       "              [55, 58, 'BCV', 'SPEC'],\n",
       "              [181, 192, 'coronavirus', 'SPEC']]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Species_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train, test and dev pmc ids\n",
    "import math\n",
    "import random\n",
    "import jsonlines\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "\n",
    "\n",
    "def get_train_dev_test_indxs(total_num_annotations):\n",
    "\n",
    "    percentage=0.70\n",
    "    iter = 0\n",
    "\n",
    "    trainids = []\n",
    "    devids = []\n",
    "    testids =[]\n",
    "\n",
    "    nLines = total_num_annotations\n",
    "    nTrain = int(nLines*percentage) \n",
    "    nValid = math.floor((nLines - nTrain)/2)\n",
    "    nTest = nLines - (nTrain+nValid)\n",
    "\n",
    "    deck = list(range(0, nLines))\n",
    "    random.seed(45) # This will be fixed for reproducibility\n",
    "    random.shuffle(deck)\n",
    "\n",
    "    train_ids = deck[0:nTrain]\n",
    "    devel_ids = deck[nTrain:nTrain+nValid]\n",
    "    test_ids = deck[nTrain+nValid:nTrain+nValid+nTest]\n",
    "\n",
    "    return train_ids, devel_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_jsonl_format(dictionary_dataset, train_ids, devel_ids, test_ids,path, folder_name): \n",
    "    \n",
    "    result_path = cwd_+'BIO/non_overlapping_dataset/'\n",
    "    pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_jsonl_data = []\n",
    "    devel_jsonl_data=[]\n",
    "    test_jsonl_data = []\n",
    "    \n",
    "    iter = 1\n",
    "    \n",
    "    for key, values in dictionary_dataset.items():\n",
    "        \n",
    "        text = str(key)\n",
    "        \n",
    "        for each_ner in values:\n",
    "            point_start = int(each_ner[0])\n",
    "            point_end = int(each_ner[1])\n",
    "            label = each_ner[4]\n",
    "            entities.append((point_start, point_end,label))\n",
    "        \n",
    "        if iter in train_ids:\n",
    "            train_jsonl_data.append((text, {\"entities\" : entities}))\n",
    "        elif iter in devel_ids:\n",
    "            devel_jsonl_data.append((text, {\"entities\" : entities}))\n",
    "        elif iter in test_ids:\n",
    "            test_jsonl_data.append((text, {\"entities\" : entities}))            \n",
    "    \n",
    "        iter = iter+1\n",
    "        \n",
    "    with jsonlines.open(path+'train.json', mode='w') as writer:\n",
    "        for each_line in tqdm(train_jl):\n",
    "            writer.write(each_line)\n",
    "\n",
    "    with jsonlines.open(path+'devel.json', mode='w') as writer:\n",
    "        for each_line in tqdm(dev_jl):\n",
    "            writer.write(each_line)\n",
    "            \n",
    "    with jsonlines.open(path+'test.json', mode='w') as writer:\n",
    "        for each_line in tqdm(test_jl):\n",
    "            writer.write(each_line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # if a sub span is present in full span return it\n",
    "    if sub_span_range[0] in range(full_spans_range[0],full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "\n",
    "    \n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "\n",
    "    print(text_data, ner_tags)\n",
    "    tokens = []\n",
    "    ners = []\n",
    "    spans = []\n",
    "\n",
    "    split_text = tokenizer.tokenize(text_data)\n",
    "    span_text = list(tokenizer.span_tokenize(text_data))\n",
    "    # for each word token append 'O'\n",
    "    arr = ['O'] * len(split_text)\n",
    "\n",
    "    for each_tag in ner_tags:\n",
    "        span_list = (int(each_tag[0]), int(each_tag[1]))\n",
    "        token_list = wordpunct_tokenize(each_tag[2])\n",
    "        ner_list = wordpunct_tokenize(each_tag[3])\n",
    "\n",
    "        if (len(token_list) > len(ner_list)):\n",
    "            ner_list = len(token_list) * ner_list\n",
    "        for i in range(0, len(ner_list)):\n",
    "            # The logic here is look for the first B-tag and then append I-tag next\n",
    "            if (i == 0):\n",
    "                ner_list[i] = 'B-' + ner_list[i]\n",
    "            else:\n",
    "                ner_list[i] = 'I-' + ner_list[i]\n",
    "\n",
    "        tokens.append(token_list)\n",
    "        ners.append(ner_list)\n",
    "        spans.append(span_list)\n",
    "\n",
    "    split_token_span_list = list(zip(split_text, span_text))\n",
    "    span_ner_list = list(zip(spans, ners))\n",
    "\n",
    "    \n",
    "    sub_spans =[] # get sub spans from the full spans of the ner\n",
    "\n",
    "    for each_span_ner_list in span_ner_list:\n",
    "        # in full range ner e.g., [144, 150, 'COVID-19', 'DISO']\n",
    "        count = 0\n",
    "        # count is to keep track of the B, I, sub tags in the ner list\n",
    "        for each_token in split_token_span_list:\n",
    "            sub_spans_ = find_sub_span(each_token[1], each_span_ner_list[0])\n",
    "            if sub_spans_:\n",
    "                sub_spans.append([sub_spans_,each_span_ner_list[1][count]])\n",
    "                count = count+1\n",
    "            \n",
    "            \n",
    "    \n",
    "    for i, each_span_token in enumerate(split_token_span_list):\n",
    "        for each_ner_span in sub_spans:\n",
    "            if each_span_token[1] == each_ner_span[0]:\n",
    "                arr[i] = ''.join(each_ner_span[1])\n",
    "\n",
    "    return zip(split_text, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_IOB_format(dictionary_dataset, train_ids, devel_ids, test_ids, path, folder_name):\n",
    "    \n",
    "    result_path = cwd_+'BIO/non_overlapping_dataset/'\n",
    "    pathlib.Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(result_path + 'train.csv', 'w', newline='\\n') as f1, open(result_path + 'devel.csv', 'w',\n",
    "                        newline='\\n') as f2, open(result_path + 'test.csv', 'w', newline='\\n') as f3:\n",
    "\n",
    "        train_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "        dev_writer = csv.writer(f2, delimiter='\\t', lineterminator='\\n')\n",
    "        test_writer = csv.writer(f3, delimiter='\\t', lineterminator='\\n')\n",
    "        \n",
    "        iter = 0\n",
    "\n",
    "        for key, values in dictionary_dataset.items():\n",
    "            \n",
    "            text = str(key)\n",
    "#             print(text)\n",
    "            tagged_tokens = convert2IOB(text, values)\n",
    "\n",
    "\n",
    "            if iter in train_ids:\n",
    "                for each_token in tagged_tokens:\n",
    "                    train_writer.writerow(list(each_token))\n",
    "                train_writer.writerow('')\n",
    "\n",
    "            elif iter in devel_ids:\n",
    "                for each_token in tagged_tokens:\n",
    "                    dev_writer.writerow(list(each_token))\n",
    "                dev_writer.writerow('')\n",
    "\n",
    "            elif iter in test_ids:\n",
    "                for each_token in tagged_tokens:\n",
    "                    test_writer.writerow(list(each_token))\n",
    "                test_writer.writerow('')\n",
    "            \n",
    "            iter = iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/santosh/Work/GitHub/COVID-19-Named-Entity-Recognition-/Datasets/\n"
     ]
    }
   ],
   "source": [
    "cwd_ = os.getcwd()+'/Datasets/'\n",
    "print(cwd_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE:\n",
      "Identification of a bovine coronavirus packaging signal.\n",
      "\n",
      " [[27, 33, 'bovine', 'SPEC'], [34, 45, 'coronavirus', 'SPEC']]\n",
      "ABSTRACT:\n",
      "A region of the bovine coronavirus (BCV) genome that functions as a packaging signal has been cloned. [[26, 32, 'bovine', 'SPEC'], [33, 44, 'coronavirus', 'SPEC'], [46, 49, 'BCV', 'SPEC']]\n",
      "The 291-nucleotide clone shares 72% homology with the region of mouse hepatitis coronavirus (MHV) gene 1b that contains the packaging signal. [[8, 18, 'nucleotide', 'CHED'], [64, 79, 'mouse hepatitis', 'DISO'], [80, 91, 'coronavirus', 'SPEC'], [93, 96, 'MHV', 'SPEC']]\n",
      "RNA transcripts were packaged into both BCV and MHV virions when the cloned region was appended to a noncoronavirus RNA. [[40, 43, 'BCV', 'SPEC'], [48, 51, 'MHV', 'SPEC'], [52, 59, 'virions', 'COMP']]\n",
      "This is the first identification of a BCV packaging signal. [[38, 41, 'BCV', 'SPEC']]\n",
      "The data demonstrate that the BCV genome contains a sequence that is conserved at both the sequence and functional levels, thus broadening our insight into coronavirus packaging.\n",
      "\n",
      "\n",
      " [[30, 33, 'BCV', 'SPEC'], [156, 167, 'coronavirus', 'SPEC']]\n",
      "TITLE:\n",
      "Comparison of systemic cytokine levels in patients with acute respiratory distress syndrome, severe pneumonia, and controls.\n",
      "\n",
      " [[27, 33, 'bovine', 'SPEC'], [34, 45, 'coronavirus', 'SPEC'], [91, 97, 'bovine', 'SPEC'], [98, 109, 'coronavirus', 'SPEC'], [111, 114, 'BCV', 'SPEC']]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-59f3117a24e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_dev_test_indxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_overlapping_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconvert_to_IOB_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_overlapping_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BIO/non_overlapping_dataset/'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconvert_to_jsonl_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_overlapping_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SpaCy/non_overlapping_dataset/'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-439973ade59b>\u001b[0m in \u001b[0;36mconvert_to_IOB_format\u001b[0;34m(dictionary_dataset, train_ids, devel_ids, test_ids, path, folder_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#             print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert2IOB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6e1f7336cbbe>\u001b[0m in \u001b[0;36mconvert2IOB\u001b[0;34m(text_data, ner_tags)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0msub_spans_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_sub_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_span_ner_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msub_spans_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0msub_spans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub_spans_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meach_span_ner_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# non_overlapping_dataset \n",
    "\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(non_overlapping_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/non_overlapping_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/non_overlapping_dataset/' )\n",
    "\n",
    "\n",
    "# overlapping_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(overlapping_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/overlapping_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/overlapping_dataset/' )\n",
    "\n",
    "\n",
    "# Disorder_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Disorder_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Disorder_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Disorder_dataset/' )\n",
    "\n",
    "# Species_dataset\n",
    "\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Species_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Species_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Species_dataset/' )\n",
    "\n",
    "# Chemical_Drug_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Chemical_Drug_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Chemical_Drug_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Chemical_Drug_dataset/' )\n",
    "\n",
    "# Gene_Protein_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Gene_Protein_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Gene_Protein_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Gene_Protein_dataset/' )\n",
    "\n",
    "# Enzyme_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Enzyme_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Enzyme_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Enzyme_dataset/' )\n",
    "\n",
    "# Anatomy_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Anatomy_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Anatomy_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Anatomy_dataset/' )\n",
    "\n",
    "# Biological_Process_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Biological_Process_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Biological_Process_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Biological_Process_dataset/' )\n",
    "\n",
    "# Molecular_Function_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Molecular_Function_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Molecular_Function_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Molecular_Function_dataset/' )\n",
    "\n",
    "# Cellular_Component_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Cellular_Component_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Cellular_Component_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Cellular_Component_dataset/' )\n",
    "\n",
    "# Pathway_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(Pathway_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/Pathway_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/Pathway_dataset/' )\n",
    "\n",
    "# microRNA_dataset\n",
    "trainidx, develidx, testidx = get_train_dev_test_indxs(len(microRNA_dataset))\n",
    "convert_to_IOB_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'BIO/microRNA_dataset/' )\n",
    "convert_to_jsonl_format(non_overlapping_dataset,trainidx, develidx, testidx, cwd_, 'SpaCy/microRNA_dataset/' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coun = 0\n",
    "for j,v in Species_dataset.items():\n",
    "    print(j,v)\n",
    "    if coun ==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "split_text = tokenizer.tokenize(str(j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scispacy]",
   "language": "python",
   "name": "conda-env-scispacy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
